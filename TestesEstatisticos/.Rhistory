library("scmamp")
library(dplyr)
# Constantes
AUC = "Area under the curve"
KSVM = "classif.ksvm"
RF = "classif.randomForest"
XGB = "classif.xgboost"
SMOTE = "SMOTE"
ADASYN = "ADASYN"
MEASURE = AUC
ds = read.csv("../SummaryResults/summary_compilation_fixed.csv", header = T)
summary(ds)
ds = group_by(ds, learner , weight_space , measure , sampling , ruspool , dataset , imba.rate)
ds = summarise(ds, tuning_measure = mean(tuning_measure), holdout_measure = mean(holdout_measure), holdout_measure_residual = mean(holdout_measure_residual))
ds = as.data.frame(ds)
ds = filter(ds, measure == MEASURE, imba.rate == DESB)
# Dividindo o ds em n, um para cada técnica
splited_df = ds %>% group_by(learner, sampling, ruspool, weight_space) %>% do(vals = as.data.frame(.)) %>% select(vals) %>% sapply(function(x){x})
# Legenda
splited_df[[1]][1,]
splited_df[[2]][1,]
splited_df[[3]][1,]
splited_df[[4]][1,]
splited_df[[5]][1,]
splited_df[[6]][1,]
splited_df[[7]][1,]
splited_df[[9]][1,]
splited_df[[10]][1,]
splited_df[[11]][1,]
splited_df[[12]][1,]
splited_df[[13]][1,]
splited_df[[14]][1,]
splited_df[[15]][1,]
# Juntando cada uma das partes horizontalmente em um data set
df_tec_wide = do.call("cbind", splited_df)
# Renomeando duplicacao de nomes
colnames(df_tec_wide) = make.unique(colnames(df_tec_wide))
# Selecionando apenas as medidas de holdout_measure_residual
df_tec_wide_residual = select(df_tec_wide, starts_with("holdout_measure_residual"))
# Renomeando colunas para ids
print("length(colnames(df_tec_wide_residual))")
print(length(colnames(df_tec_wide_residual)))
colnames(df_tec_wide_residual) = c(1:length(colnames(df_tec_wide_residual)))
# Verificando a dimensao do df
dim(df_tec_wide_residual)
# Removendo linhas com NA's
df_tec_wide_residual = na.omit(df_tec_wide_residual)
# Renomeando a variavel
df = df_tec_wide_residual
summary(df)
library("scmamp")
plotDensities(data = df)
friedmanTest(df)
test <- nemenyiTest (df, alpha=0.05)
abs(test$diff.matrix) > test$statistic
ds = filter(ds, measure == MEASURE, imba.rate == DESB)
ds = read.csv("../SummaryResults/summary_compilation_fixed.csv", header = T)
summary(ds)
# Dividindo o ds em n, um para cada técnica
splited_df = ds %>% group_by(learner) %>% do(vals = as.data.frame(.)) %>% select(vals) %>% sapply(function(x){x})
# Legenda
splited_df[[1]][1,]
splited_df[[2]][1,]
splited_df[[3]][1,]
# Juntando cada uma das partes horizontalmente em um data set
df_tec_wide = do.call("cbind", splited_df)
# Renomeando duplicacao de nomes
colnames(df_tec_wide) = make.unique(colnames(df_tec_wide))
# Selecionando apenas as medidas de holdout_measure_residual
df_tec_wide_residual = select(df_tec_wide, starts_with("holdout_measure_residual"))
# Renomeando colunas para ids
colnames(df_tec_wide_residual) = c(1:length(colnames(df_tec_wide_residual)))
# Verificando a dimensao do df
dim(df_tec_wide_residual)
# Removendo linhas com NA's
df_tec_wide_residual = na.omit(df_tec_wide_residual)
# Renomeando a variavel
df = df_tec_wide_residual
summary(df)
library("scmamp")
library(dplyr)
# Constantes
AUC = "Area under the curve"
KSVM = "classif.ksvm"
RF = "classif.randomForest"
XGB = "classif.xgboost"
SMOTE = "SMOTE"
ADASYN = "ADASYN"
MEASURE = AUC
ds = read.csv("../SummaryResults/summary_compilation_fixed.csv", header = T)
summary(ds)
ds = group_by(ds, learner , weight_space , measure , sampling , ruspool , dataset , imba.rate)
ds = summarise(ds, tuning_measure = mean(tuning_measure), holdout_measure = mean(holdout_measure), holdout_measure_residual = mean(holdout_measure_residual))
ds = as.data.frame(ds)
ds = filter(ds, measure == MEASURE, imba.rate == DESB)
# Dividindo o ds em n, um para cada técnica
splited_df = ds %>% group_by(learner) %>% do(vals = as.data.frame(.)) %>% select(vals) %>% sapply(function(x){x})
# Legenda
splited_df[[1]][1,]
splited_df[[2]][1,]
splited_df[[3]][1,]
# Juntando cada uma das partes horizontalmente em um data set
df_tec_wide = do.call("cbind", splited_df)
# Renomeando duplicacao de nomes
colnames(df_tec_wide) = make.unique(colnames(df_tec_wide))
# Selecionando apenas as medidas de holdout_measure_residual
df_tec_wide_residual = select(df_tec_wide, starts_with("holdout_measure_residual"))
# Renomeando colunas para ids
colnames(df_tec_wide_residual) = c(1:length(colnames(df_tec_wide_residual)))
# Verificando a dimensao do df
dim(df_tec_wide_residual)
# Removendo linhas com NA's
df_tec_wide_residual = na.omit(df_tec_wide_residual)
# Renomeando a variavel
df = df_tec_wide_residual
summary(df)
library("scmamp")
plotDensities(data = df)
friedmanTest(df)
test <- nemenyiTest (df, alpha=0.05)
abs(test$diff.matrix) > test$statistic
plotCD(df, alpha=0.05, cex=0.55)
group_by(ds, learner)
group_by(ds, "learner")
group_by(ds, "learnergfd")
group_by(ds, c(learner))
group_by(ds, c(measure))
summary(ds)
group_by(ds, c(sampling))
group_by(ds, c(sampling, measure))
group_by(ds, vars = c("sampling", "measure"))
group_by(ds, vars = vars(c("sampling", "measure")))
group_by(ds, vars(c("sampling", "measure")))
group_by(ds, vars(one_of(c("sampling", "measure"))))
vars(oi)
?vars
vars(c(sampling, measure))
group_by(ds, vars(c(sampling, measure)))
group_by(ds, vars = vars(c(sampling, measure)))
vars(c(sampling, measure))
vars(c("sampling", "measure"))
v = vars(c("sampling", "measure"))
group_by(ds, v)
group_by(ds, vars(c("sampling", "measure")))
?one_of
group_by(ds, vars(one_of(c("sampling", "measure"))))
vars(one_of(c("sampling", "measure")))
group = c("sampling", "measure"))
group = c("sampling", "measure")
select(ds, group)
group_by(ds, group)
group_by(ds, vars= group)
?group_by
?group_by_at
group_by_at(ds, vars= group)
group_by_at(ds, .vars= group)
# Legenda
splited_df[[1]][1,]
splited_df[[2]][1,]
splited_df[[3]][1,]
splited_df[[4]][1,]
library("scmamp")
library(dplyr)
# Constantes
AUC = "Area under the curve"
KSVM = "classif.ksvm"
RF = "classif.randomForest"
XGB = "classif.xgboost"
SMOTE = "SMOTE"
ADASYN = "ADASYN"
# Identificadores
ALGO = "learner"
TECN = c("sampling", "weight_space", "ruspool")
MEASURE = AUC
COLUMNS = c(ALGO, TECN)
ds = read.csv("../SummaryResults/summary_compilation_fixed.csv", header = T)
summary(ds)
ds = group_by(ds, learner , weight_space , measure , sampling , ruspool , dataset , imba.rate)
ds = summarise(ds, tuning_measure = mean(tuning_measure), holdout_measure = mean(holdout_measure), holdout_measure_residual = mean(holdout_measure_residual))
ds = as.data.frame(ds)
ds = filter(ds, measure == MEASURE, imba.rate == DESB)
# Dividindo o ds em n, um para cada técnica
splited_df = ds %>% group_by(learner, sampling, ruspool, weight_space) %>% do(vals = as.data.frame(.)) %>% select(vals) %>% sapply(function(x){x})
# Juntando cada uma das partes horizontalmente em um data set
df_tec_wide = do.call("cbind", splited_df)
# Renomeando duplicacao de nomes
colnames(df_tec_wide) = make.unique(colnames(df_tec_wide))
# Selecionando apenas as medidas de holdout_measure_residual
df_tec_wide_residual = select(df_tec_wide, starts_with("holdout_measure_residual"))
# Renomeando colunas para ids
colnames(df_tec_wide_residual) = c(1:length(colnames(df_tec_wide_residual)))
# Verificando a dimensao do df
dim(df_tec_wide_residual)
# Removendo linhas com NA's
df_tec_wide_residual = na.omit(df_tec_wide_residual)
# Renomeando a variavel
df = df_tec_wide_residual
summary(df)
library("scmamp")
plotDensities(data = df)
friedmanTest(df)
test <- nemenyiTest (df, alpha=0.05)
abs(test$diff.matrix) > test$statistic
# Legenda
splited_df[[1]][1,]
splited_df[[2]][1,]
splited_df[[3]][1,]
splited_df[[4]][1,]
splited_df[[5]][1,]
splited_df[[6]][1,]
splited_df[[7]][1,]
splited_df[[9]][1,]
splited_df[[10]][1,]
splited_df[[11]][1,]
splited_df[[12]][1,]
splited_df[[13]][1,]
splited_df[[14]][1,]
splited_df[[15]][1,]
library("scmamp")
library(dplyr)
# Constantes
AUC = "Area under the curve"
KSVM = "classif.ksvm"
RF = "classif.randomForest"
XGB = "classif.xgboost"
SMOTE = "SMOTE"
ADASYN = "ADASYN"
# Identificadores
ALGO = "learner"
TECN = c("sampling", "weight_space", "ruspool")
MEASURE = AUC
COLUMNS = c(ALGO, TECN)
ds = read.csv("../SummaryResults/summary_compilation_fixed.csv", header = T)
summary(ds)
ds = group_by(ds, learner , weight_space , measure , sampling , ruspool , dataset , imba.rate)
ds = summarise(ds, tuning_measure = mean(tuning_measure), holdout_measure = mean(holdout_measure), holdout_measure_residual = mean(holdout_measure_residual))
ds = as.data.frame(ds)
ds = filter(ds, measure == MEASURE, imba.rate == DESB)
# Dividindo o ds em n, um para cada técnica
splited_df = ds %>% group_by(learner, sampling, ruspool, weight_space) %>% do(vals = as.data.frame(.)) %>% select(vals) %>% sapply(function(x){x})
# Juntando cada uma das partes horizontalmente em um data set
df_tec_wide = do.call("cbind", splited_df)
# Renomeando duplicacao de nomes
colnames(df_tec_wide) = make.unique(colnames(df_tec_wide))
# Selecionando apenas as medidas de holdout_measure_residual
df_tec_wide_residual = select(df_tec_wide, starts_with("holdout_measure_residual"))
# Renomeando colunas para ids
colnames(df_tec_wide_residual) = c(1:length(colnames(df_tec_wide_residual)))
# Verificando a dimensao do df
dim(df_tec_wide_residual)
# Removendo linhas com NA's
df_tec_wide_residual = na.omit(df_tec_wide_residual)
# Renomeando a variavel
df = df_tec_wide_residual
summary(df)
library("scmamp")
plotDensities(data = df)
friedmanTest(df)
test <- nemenyiTest (df, alpha=0.05)
abs(test$diff.matrix) > test$statistic
# Legenda
splited_df[[1]][1,]
splited_df[[2]][1,]
splited_df[[3]][1,]
splited_df[[4]][1,]
splited_df[[5]][1,]
splited_df[[6]][1,]
splited_df[[7]][1,]
splited_df[[9]][1,]
splited_df[[10]][1,]
splited_df[[11]][1,]
splited_df[[12]][1,]
splited_df[[13]][1,]
splited_df[[14]][1,]
splited_df[[15]][1,]
plotCD(df, alpha=0.05, cex=0.45)
# Legenda
splited_df[[1]][1,]
splited_df[[2]][1,]
splited_df[[3]][1,]
splited_df[[4]][1,]
splited_df[[5]][1,]
splited_df[[6]][1,]
splited_df[[7]][1,]
splited_df[[9]][1,]
splited_df[[10]][1,]
splited_df[[11]][1,]
splited_df[[12]][1,]
splited_df[[13]][1,]
splited_df[[14]][1,]
splited_df[[15]][1,]
library("scmamp")
library(dplyr)
# Constantes
AUC = "Area under the curve"
KSVM = "classif.ksvm"
RF = "classif.randomForest"
XGB = "classif.xgboost"
SMOTE = "SMOTE"
ADASYN = "ADASYN"
# Identificadores
ALGO = "learner"
TECN = c("sampling", "weight_space", "ruspool")
MEASURE = AUC
COLUMNS = c(ALGO, TECN)
ds = read.csv("../SummaryResults/summary_compilation_fixed.csv", header = T)
summary(ds)
ds = group_by(ds, learner , weight_space , measure , sampling , ruspool , dataset , imba.rate)
ds = summarise(ds, tuning_measure = mean(tuning_measure), holdout_measure = mean(holdout_measure), holdout_measure_residual = mean(holdout_measure_residual))
ds = as.data.frame(ds)
ds = filter(ds, measure == MEASURE, imba.rate == DESB)
# Dividindo o ds em n, um para cada técnica
splited_df = ds %>% group_by(learner, sampling, ruspool, weight_space) %>% do(vals = as.data.frame(.)) %>% select(vals) %>% sapply(function(x){x})
# Juntando cada uma das partes horizontalmente em um data set
df_tec_wide = do.call("cbind", splited_df)
# Renomeando duplicacao de nomes
colnames(df_tec_wide) = make.unique(colnames(df_tec_wide))
# Selecionando apenas as medidas de holdout_measure_residual
df_tec_wide_residual = select(df_tec_wide, starts_with("holdout_measure_residual"))
# Renomeando colunas para ids
colnames(df_tec_wide_residual) = c(1:length(colnames(df_tec_wide_residual)))
# Verificando a dimensao do df
dim(df_tec_wide_residual)
# Removendo linhas com NA's
df_tec_wide_residual = na.omit(df_tec_wide_residual)
# Renomeando a variavel
df = df_tec_wide_residual
summary(df)
library("scmamp")
plotDensities(data = df)
friedmanTest(df)
test <- nemenyiTest (df, alpha=0.05)
abs(test$diff.matrix) > test$statistic
# Legenda
splited_df[[1]][1,]
splited_df[[2]][1,]
splited_df[[3]][1,]
splited_df[[4]][1,]
splited_df[[5]][1,]
splited_df[[6]][1,]
splited_df[[7]][1,]
splited_df[[9]][1,]
splited_df[[10]][1,]
splited_df[[11]][1,]
splited_df[[12]][1,]
splited_df[[13]][1,]
splited_df[[14]][1,]
splited_df[[15]][1,]
plotCD(df, alpha=0.05, cex=0.45)
# Legenda
for(i in (1:length(splited_df))){
splited_df[[i]][1,]
}
source('~/.active-rstudio-document', echo=TRUE)
ds
summary(ds)
levels(ds['dataset'])
str(ds['dataset'])
ds = filter_at(ds, .vars = FILTER_KEY, .vars_predicate = any_vars(. == FILTER_VALUE))
library(dplyr)
ds = filter_at(ds, .vars = FILTER_KEY, .vars_predicate = any_vars(. == FILTER_VALUE))
rmd = ./Dropbox/UNICAMP/IC/estudo_cost_learning/TestesEstatisticos/(Algo+BD+0.05)x(Tecnica).Rmd
rmd = "./Dropbox/UNICAMP/IC/estudo_cost_learning/TestesEstatisticos/(Algo+BD+0.05)x(Tecnica).Rmd"
rmarkdown::render(rmd, params = list( columns = c("weight_space", "sampling", "ruspool")))
rmarkdown::render(rmd, params = list( columns = c("weight_space", "sampling", "ruspool")))
rmarkdown::render(rmd, params = list( columns = c("weight_space", "sampling", "ruspool")))
rmarkdown::render(rmd, params = list( columns = c("weight_space", "sampling", "ruspool")))
rmarkdown::render(rmd, params = list( columns = c("weight_space", "sampling", "ruspool")))
rmarkdown::render(rmd, params = list( columns = "weight_space"))
rmarkdown::render(rmd, params = list( columns = "weight_space"))
rmarkdown::render(rmd, params = list( columns = "weight_space"))
rmarkdown::render(rmd, params = list( columns = "weight_space"))
rmarkdown::render(rmd, params = list( columns = "weight_space"))
rmarkdown::render(rmd)
?rmarkdown::rneder
?rmarkdown::render
rmarkdown::render(rmd)
rmarkdown::render(rmd)
rmarkdown::render(rmd)
rmarkdown::render(rmd)
assign("last.warning", NULL, envir = baseenv())
rmarkdown::render(rmd)
rmarkdown::render(rmd)
rmarkdown::render(rmd)
rmarkdown::render(rmd)
rmarkdown::render(rmd)
rmarkdown::render(rmd)
rmarkdown::render(rmd, params = list(columns = c("sampling")))
rmarkdown::render(rmd, params = list(columns = c("sampling", "class_weight")))
rmarkdown::render(rmd, params = list(columns = c("sampling", "class_weight")))
\
rmarkdown::render(rmd, params = list(columns = c("sampling", "class_weight")))
rmarkdown::render(rmd, params = list(columns = c("sampling", "class_weight")))
rmarkdown::render(rmd, params = list(columns = c("sampling", "weight_space")))
rmarkdown::render(rmd, params = list(columns = c("sampling", "weight_space")))
rmarkdown::render(rmd, params = list(columns = c("sampling", "weight_space")))
rmarkdown::render(rmd, params = list(columns = c("sampling", "weight_space", "ruspool")))
rmarkdown::render(rmd, params = list(columns = c("sampling", "weight_space", "ruspool")))
rmarkdown::render(rmd, params = list(columns = c("sampling", "weight_space", "ruspool"), filter_key = c("imba.rate"), filter_values = c("0.05")))
rmarkdown::render(rmd, params = list(columns = c("sampling", "weight_space", "ruspool"), filter_keys = c("imba.rate"), filter_values = c("0.05")))
rmarkdown::render(rmd, params = list(columns = c("sampling", "weight_space", "ruspool"), measure = "Accuracy"))
rmarkdown::render(rmd, params = list(columns = c("sampling", "weight_space", "ruspool"), measure = "Accuracy", filter_keys=c("oi")))
rmarkdown::render(rmd, params = list(columns = c("sampling", "weight_space", "ruspool"), measure = c("Accuracy", "MCC")))
rmarkdown::render(rmd, params = list(columns = c("sampling", "weight_space", "ruspool"), measure = "Accuracy", filter_keys=c("oi")))
rmarkdown::render(rmd, params = list(columns = c("sampling", "weight_space", "ruspool"), measure = "Accuracy")
)
)
rmarkdown::render(rmd, params = list(columns = c("sampling", "weight_space", "ruspool"), measure = "Accuracy"))
rmarkdown::render(rmd, params = list(columns = c("sampling", "weight_space", "ruspool"), measure = "Accuracy", filter_keys=c("imba.rate"), filter_values=c("0.05")))
rmarkdown::render(rmd, params = list(columns = c("sampling", "weight_space", "ruspool"), measure = "Accuracy", filter_keys=c("imba.rate"), filter_values=c("0.05")))
rmarkdown::render(rmd, params = list(columns = c("sampling", "weight_space", "ruspool"), measure = "Accuracy", filter_keys=c("imba.rate"), filter_values=c("0.05"), performance = "holdout_measure_residual"))
rmarkdown::render(rmd, params = list(columns = c("sampling", "weight_space", "ruspool"), measure = "Accuracy", filter_keys=c("imba.rate"), filter_values=c("0.05"), performance = "holdout_measure_residualsd"))
rmarkdown::render(rmd, params = list(columns = c("sampling", "weight_space", "ruspool"), measure = "Accuracy", filter_keys=c("imba.rate"), filter_values=c("0.05"), performance = "holdout_measure_residua"))
rmarkdown::render(rmd, params = list(columns = c("sampling", "weight_space", "ruspool"), measure = "Accuracy", filter_keys=c("imba.rate"), filter_values=c("0.05"), performance = "holdout_measure_residuals"))
rmarkdown::render(rmd, params = list(columns = c("sampling", "weight_space", "ruspool"), measure = "Accuracy", filter_keys=c("imba.rate"), filter_values=c("0.05"), performance = "holdout_measure_residual"))
rmarkdown::render(rmd, params = list(columns = c("sampling", "weight_space", "ruspool"), measure = "Accuracy", filter_keys=c("imba.rate"), filter_values=c("0.05"), performance = "holdout_measure_residual"))
rmarkdown::render(rmd, params = list(columns = c("sampling", "weight_space", "ruspool"), measure = "Accuracy", filter_keys=c("imba.rate"), filter_values=c("0.05"), performance = "holdout_measure_residuals"))
rmarkdown::render(rmd, params = list(columns = c("sampling", "weight_space", "ruspool"), measure = "Accuracy", filter_keys=c("imba.rate"), filter_values=c("0.05"), performance = "holdout_measure_residua"))
?matches
?matches
rmarkdown::render(rmd, params = list(columns = c("sampling", "weight_space", "ruspool"), measure = "Accuracy", filter_keys=c("imba.rate"), filter_values=c("0.05"), performance = "holdout_measure_residua"))
rmarkdown::render(rmd, params = list(columns = c("sampling", "weight_space", "ruspool"), measure = "Accuracy", filter_keys=c("imba.rate"), filter_values=c("0.05"), performance = "holdout_measure_residual"))
select(ds, "holdout_measure")
head(select(ds, "holdout_measure"))
head(select(ds, "holdout_measure_residual"))
head(select(ds, "holdout_measure_residuals"))
rmarkdown::render(rmd, params = list(columns = c("sampling", "weight_space", "ruspool"), measure = "Accuracy", filter_keys=c("imba.rate"), filter_values=c("0.05"), performance = "holdout_measure_residual"))
d = ds
d = d[,1:@]
d = d[,1:2]
d
head(d)
colnames(d) = c("oi", "oi")
head(d)
select(d, "oi")
class(d)
select(d, oi)
?select
class(ds)
colnames(d) = c("oi", "oi2")
select(d, oi)
select(d, "oi")
colnames(d) = c("oi", "oi")
head(d)
select(d, oi)
select(d, starts_with(oi))
select(d, starts_with("oi"))
colnames(d) = make.unique(colnames(d))
d
head(d)
select(ds, matches("holdout_measure"))
head(select(ds, matches("holdout_measure")))
head(select(ds, matches("holdout_measure.")))
rmarkdown::render(rmd, params = list(columns = c("sampling", "weight_space", "ruspool"), measure = "Accuracy", filter_keys=c("imba.rate"), filter_values=c("0.05"), performance = "holdout_measure_residual"))
rmarkdown::render(rmd, params = list(columns = c("sampling", "weight_space", "ruspool"), measure = "Accuracy", filter_keys=c("imba.rate"), filter_values=c("0.05"), performance = "holdout_measure_residual"))
d
head(d)
head(select(ds, matches("holdout_measure(.[1-9])?")))
head(select(ds, matches("^holdout_measure(.[1-9])?$")))
head(select(ds, matches("^holdout_measure_residual(.[1-9])?$")))
head(select(ds, matches("^holdout_measure(.[1-9])?$")))
head(select(d, matches("^oi(.[1-9])?$")))
rmarkdown::render(rmd, params = list(columns = c("sampling", "weight_space", "ruspool"), measure = "Accuracy", filter_keys=c("imba.rate"), filter_values=c("0.05"), performance = "holdout_measure_residual"))
rmarkdown::render(rmd, params = list(columns = c("sampling", "weight_space", "ruspool"), measure = "Accuracsy", filter_keys=c("imba.rate"), filter_values=c("0.05"), performance = "holdout_measure_residual"))
RMD_FILE = "/home/rodrigo/Dropbox/UNICAMP/IC/estudo_cost_learning/TestesEstatisticos/statistical_comparisons.Rmd"
ACC = "Accuracy"
MCC = "Matthews correlation coefficient"
AUC = "Area under the curve"
GMEAN = "G-mean"
F1 = "F1 measure"
TECNICAS = c("sampling", "weight_space", "underbagging")
HOLDOUT = "holdout_measure"
RESIDUAL = "holdout_measure_residual"
TUNING = "tuning_measure"
rm(params)
for (performance in c(HOLDOUT, RESIDUAL, TUNING)){
dir.create(paste("./outputs/", performance, "/", sep=""), showWarnings = FALSE)
for (measure in c(ACC, AUC, MCC, GMEAN, F1)){
dir.create(paste("./outputs/", performance, "/", measure, "/", sep=""), showWarnings = FALSE)
folder = paste("./outputs/", performance, "/", measure, "/", sep="")
# 1-) (Algo+BD+desb)x(Tecnica)
rmarkdown::render(RMD_FILE,
params = list(columns = TECNICAS,
measure = measure,
performance = performance),
output_file = paste(folder, "Algo+BD+desb(-VS-)Tecnica", performance, measure, ".pdf", sep="_"))
# 2-) (Algo+BD+desb_0.05)x(Tecnica)
for(imba in c("0.05", "0.03", "0.01", "0.001")){
rmarkdown::render(RMD_FILE,
params = list(columns = TECNICAS,
measure = measure,
filter_keys = c("imba.rate"),
filter_values = c(imba),
performance = performance),
output_file = paste(folder, "Algo+BD", imba, "(-VS-)Tecnica", performance, measure, ".pdf", sep="_"))
}
# 3-) (BD+desb+tecnica_normal)x(Algo)
rmarkdown::render(RMD_FILE,
params = list(columns = c("learner"),
measure = measure,
filter_keys = c(TECNICAS),
filter_values = c("FALSE", "FALSE", "FALSE"),
performance = performance),
output_file = paste(folder, "BD+desb+tecnica_normal(-VS-)Algo", performance, measure, ".pdf", sep="_"))
# 4-) (BD+desb)x(Algo+Tecnica)
rmarkdown::render(RMD_FILE,
params = list(columns = c(TECNICAS, "learner"),
measure = measure,
performance = performance),
output_file = paste(folder, "BD+desb(-VS-)Algo+Tecnica", performance, measure, ".pdf", sep="_"))
}
}
